%!TEX root = ms.tex
\begin{abstract}
\iffalse
Variable selection problem is popular in linear regression modeling. For each candidate model, certain predictors are restricted to have coefficients zero. In order to select the one with the best predictive performance from a sequence of candidate models, one often rely on information criteria, e.g. the squared error-based Mallows' C$_p$ and the Kullback-Leibler (KL) based AICc. Both criteria are derived under the fixed-X design. For the random-X design, Tukey's S$_p$ and the more recent RC$_p$ criteria provide unbiased estimators for the squared error-based testing error. It remains a challenge to derive a KL-based criterion for the random-X. Furthermore, in practice, we can have restrictions that are more general than setting coefficients to be zero (e.g. restricting predictors to have the same coefficient), and the goal is to select the model with the best predictive performance from a sequence of candidates each having a different set of restrictions. In this paper, we propose a KL-based information criterion under general restrictions for the random-X and we denote it as RAICc. We also extend the existing criteria (C$_p$, AICc, RC$_p$ and S$_p$) by incorporating general restrictions. We further show that the KL-based criteria (AICc and RAICc) have superior performance compared to the squared-error based information criteria and cross-validation regardless the design of X, for both variable selection and general restriction problems. Supplemental materials containing the technical details of the theorems and the complete simulation results are attached at the end of the main document. The computer code to reproduced the results for this article are available online\footnote{https://github.com/sentian/RAICc.}.
\fi

Many important modeling tasks in linear regression, including variable selection (in which slopes of some predictors are set equal to zero) and simplified models based on sums or differences of predictors (in which slopes of those predictors are set equal to (the negative of) each other), can be viewed as being based on imposing linear restrictions on regression parameters. In this paper we discuss how such models can be compared using information criteria designed to estimate predictive measures like squared error and Kullback-Leibler (KL) discrepancy in the presence of either deterministic predictors (fixed-X) or random predictors (random-X). We extend the existing fixed-X criteria C$_p$ and AICc, and random-X criteria S$_p$ and RC$_p$, to general linear restrictions. We further propose a KL-based criterion, RAICc, under random-X for variable selection in particular and general linear restrictions. We show that the use of the KL-based criteria AICc and RAICc results in better predictive performance than the use of squared error-based criteria, including cross-validation. \textcolor{red}{The following sentences are for the arXiv version. They need to be adjusted for the journal submission.}. Supplemental materials containing the technical details of the theorems and the complete simulation results are attached at the end of the main document. The computer code to reproduced the results for this article are available online\footnote{https://github.com/sentian/RAICc.}.
\end{abstract}

\noindent%
{\it Keywords:} Random-X; Optimism; Information criteria; C$_p$; AICc

