%!TEX root = ms.tex
\begin{abstract}
\iffalse
Variable selection problem is popular in linear regression modeling. For each candidate model, certain predictors are restricted to have coefficients zero. In order to select the one with the best predictive performance from a sequence of candidate models, one often rely on information criteria, e.g. the squared error-based Mallows' C$_p$ and the Kullback-Leibler (KL) based AICc. Both criteria are derived under the fixed-X design. For the random-X design, Tukey's S$_p$ and the more recent RC$_p$ criteria provide unbiased estimators for the squared error-based test error. It remains a challenge to derive a KL-based criterion for the random-X. Furthermore, in practice, we can have restrictions that are more general than setting coefficients to be zero (e.g. restricting predictors to have the same coefficient), and the goal is to select the model with the best predictive performance from a sequence of candidates each having a different set of restrictions. In this paper, we propose a KL-based information criterion under general restrictions for the random-X and we denote it as RAICc. We also extend the existing criteria (C$_p$, AICc, RC$_p$ and S$_p$) by incorporating general restrictions. We further show that the KL-based criteria (AICc and RAICc) have superior performance compared to the squared-error based information criteria and cross-validation regardless the design of X, for both variable selection and general restriction problems. Supplemental materials containing the technical details of the theorems and the complete simulation results are attached at the end of the main document. The computer code to reproduced the results for this article are available online\footnote{https://github.com/sentian/RAICc.}.
\fi

Many important modeling tasks in linear regression, including variable selection (in which slopes of some predictors are set equal to zero) and simplified models based on sums or differences of predictors (in which slopes of those predictors are set equal to each other, or the negative of each other, respectively), can be viewed as being based on imposing linear restrictions on regression parameters. In this paper, we discuss how such models can be compared using information criteria designed to estimate predictive measures like squared error and Kullback-Leibler (KL) discrepancy, in the presence of either deterministic predictors (fixed-X) or random predictors (random-X). We extend the justifications for existing fixed-X criteria C$_p$, FPE and AICc, and random-X criteria S$_p$ and RC$_p$, to general linear restrictions. We further propose and justify a KL-based criterion, RAICc, under random-X for variable selection and general linear restrictions. We show in simulations that the use of the KL-based criteria AICc and RAICc results in better predictive performance and sparser solutions than the use of squared error-based criteria, including cross-validation. Supplemental material containing the technical details of the theorems is attached at the end of the main document. The computer code to reproduce the results for this article, and the complete set of simulation results, are available online\footnote{https://github.com/sentian/RAICc.}.
\end{abstract}

\noindent%
{\it Keywords:} AICc; C$_p$; Information criteria; Optimism; Random-X

